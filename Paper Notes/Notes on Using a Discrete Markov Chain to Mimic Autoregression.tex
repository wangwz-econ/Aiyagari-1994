\documentclass[12pt]{article}

% DEFAULT PACKAGE SETUP
\usepackage{setspace,graphicx,epstopdf,amsmath,amsfonts,amssymb,amsthm,geometry}
\usepackage{marginnote,datetime,enumitem,rotating,fancyvrb}
\usepackage{threeparttable,float,soul,booktabs}
\usdate
\geometry{scale=0.8}

% FONT
\usepackage{xeCJK,fontspec} 
\setCJKmainfont{KaiTi}
\setCJKmonofont{SimSun} 
%\usepackage{newtxtext,newtxmath} % Times New Roman
%\usepackage{newpxtext,newpxmath} % Too Slim
%\usepackage{fouriernc} 		  % Too Curved
\usepackage{fourier}    		  % Favourite Font

%% Use natbib.sty.
\usepackage{natbib,fancybox,url,graphicx,color}
\definecolor{MyBlue}{rgb}{0,0.2,0.6}
\definecolor{MyRed}{rgb}{0.4,0,0.1}
\definecolor{MyGreen}{rgb}{0,0.4,0}
\usepackage[bookmarks=true,bookmarksnumbered=true,colorlinks=true,linkcolor=MyBlue,citecolor=MyRed,filecolor=MyBlue,urlcolor=MyGreen]{hyperref}

\usdate
\geometry{scale=0.8}

%% Use natbib.sty.
\usepackage{natbib,fancybox,url,graphicx,color}
\definecolor{MyBlue}{rgb}{0,0.2,0.6}
\definecolor{MyRed}{rgb}{0.4,0,0.1}
\definecolor{MyGreen}{rgb}{0,0.4,0}
\usepackage[bookmarks=true,bookmarksnumbered=true,colorlinks=true,linkcolor=MyBlue,citecolor=MyRed,filecolor=MyBlue,urlcolor=MyGreen]{hyperref}
\bibliographystyle{aer}

%% Theorem Environment
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}{Definition} % Number definitions on their own
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newtheorem{assumption}[theorem]{Assumption}

\begin{document}
\title{\bf {Finite State Markov-Chain Approximations to Univariate and Vector Autoregressions - Economics Letters - 1986}}
\author{Wenzhi Wang \thanks{This note is written down during my M.phil. period at the University of Oxford. } }
\date{May 9, 2023}

\maketitle

\section{Introduction}

This paper develops a method for choosing values for the state variables and the transition probabilities so that the resulting finite-state Markov chain mimics closely an underlying continuous-valued autoregression.


\section{The Scalar Case}
Let $y_t$ be generated by the autoregressive scheme
\begin{equation}
	\label{2} \tag{2}
	y_t = \lambda y_{t-1} + \varepsilon_t,
\end{equation}
where $\varepsilon_t$ is a white noise process with variance $\sigma_{\varepsilon}^2$. Let the distribution function of $\varepsilon_t$ be $\operatorname{Pr}(\varepsilon_t \leq u) = F(u/\sigma_{\varepsilon})$, where $F$ is a cumulative distribution with unit variance. Let $\tilde{y}_t$ denote the discrete-valued process that approximates the continuous-valued process (\ref{2}), and let $\bar{y}^1 < \bar{y}^2 < \ldots < \bar{y}^N$ denote the values that $\tilde{y}_t$ may take on. A method for selecting the values $\bar{y}^j$ is to let $\bar{y}^N$ be a multiple $m$ of the unconditional standard deviation $\sigma_y = \sqrt{\frac{\sigma_{\varepsilon}^2}{1-\lambda^2}}$. Then let $\bar{y}^1 = -\bar{y}^N$, and let the remaining be equispaced over the interval $[\bar{y}^1, -\bar{y}^N]$.

The method for calculating the transition matrix $p_{jk} = \operatorname{Pr} \left[\tilde{y}_t = \bar{y}^k \mid \tilde{y}_{t-1} = \bar{y}^j \right]$ follows. Put $w = \bar{y}^k - \bar{y}^{k-1}$. For each $j$, if $k$ is between 2 and $N-1$, set
\begin{equation}
	\label{3a} \tag{3a}
	\begin{aligned}
		p_{j k} & =\operatorname{Pr}\left[\bar{y}^k-w / 2 \leqslant \lambda \bar{y}^j+\epsilon_t \leqslant \bar{y}^k+w / 2\right] \\
		& =F\left(\frac{\bar{y}^k-\lambda \bar{y}^j+w / 2}{\sigma_\epsilon}\right)-F\left(\frac{\bar{y}^k-\lambda \bar{y}^j-w / 2}{\sigma_\epsilon}\right),
	\end{aligned}
\end{equation}
otherwise,
\begin{equation}
	\label{3b} \tag{3b}
	p_{j 1}=F\left(\frac{\bar{y}^1-\lambda \bar{y}^j+w / 2}{\sigma_\epsilon}\right) \text { and } p_{j N}=1-F\left(\frac{\bar{y}^N-\lambda \bar{y}^j-w / 2}{\sigma_\epsilon}\right) .
\end{equation}

The rationale for this assignment of the transition probabilities can be understood by considering a random variable of the form $v = \lambda \bar{y}^j + \varepsilon$ where $\bar{y}^j $ is fixed and $\varepsilon$ is distributed as $\varepsilon_t$. Then the assignment for $p_{jk}$ make the distribution of $\tilde{y}_t$ conditional on $\tilde{y}_{t-1} = \bar{y}^j$ be a discrete approximation to the distribtuion of the random variable $v$. 

It is recognized that other integration rules, e.g., \emph{\bf Gaussian quadrature}, may lead to a placement of the grid points that in principle is more efficient than the equispaced scheme outlined above. The advantages of the above scheme, however, are computational speed and numerical stability, especially in the multivariate case given below. A rule based on Gaussian quadrature would essentially use the method of moments to determine the grid points and the transition probability matrix. This would necessarily entail the inversion of very large Vandermonde matrices, a problem which is notoriously time consuming and numerically unstable. The above scheme, on the other hand, can be coded very easily and the approximating Markov chains have been found to be quickly computable for a large number of sets of parameter values for the underlying autoregression. 

To assess the adequacy of the approximation (\ref{3a}, \ref{3b}) note that the discrete process $\tilde{y}_t$ admits a representation of the form $$\tilde{y}_t - \bar{\lambda} \tilde{y}_{t-1} = \tilde{\varepsilon}_t,$$ with $\operatorname{cov}(\tilde{\varepsilon}_t, \tilde{y}_{t-1}) = 0$, where $$\bar{\lambda} = \frac{\operatorname{cov}(\tilde{y}_{t}, \tilde{y}_{t-1})}{\operatorname{var}(\tilde{y}_{t})},$$ and $$\sigma_{\tilde{\varepsilon}_t}^2 = (1 -\bar{\lambda}^2 )\operatorname{var}(\tilde{y}_{t}).$$ The parameters $\bar{\lambda}$ and $\sigma_{\tilde{\varepsilon}_t}^2$ are functions of the second moments of the $\tilde{y}_t$ process, and these moments can be computed from the transition matrix and the $\{\bar{y}^j\}$.


\end{document}